# Convergence Rates for Stochastic Gradient Descent on Non-Convex Functions

[![Download PDF](https://img.shields.io/badge/Download-PDF-blue?logo=adobe-acrobat-reader)](https://github.com/ChicagoHAI/math-convergence-rates-for-stochast-e8d8/releases/download/latest/paper.pdf) [![Build Status](https://github.com/ChicagoHAI/math-convergence-rates-for-stochast-e8d8/actions/workflows/build-paper.yml/badge.svg)](https://github.com/ChicagoHAI/math-convergence-rates-for-stochast-e8d8/actions)

> Establishes new theoretical convergence rates for SGD on non-convex functions, showing it reaches approximate stationary points in O(1/ε^4) iterations generally and O(1/ε^2) under additional conditions.

## Abstract

We study the convergence rates of stochastic gradient descent (SGD) for non-convex optimization problems. While the behavior of SGD for convex optimization is well-understood, the non-convex setting presents unique challenges due to the presence of multiple local minima and saddle points. We establish new convergence rates under various smoothness and noise conditions, proving that SGD achieves an ε-approximate stationary point in O(1/ε^4) iterations for general non-convex functions. For functions satisfying additional regularity conditions, we demonstrate improved rates of O(1/ε^2). Our analysis provides insights into the role of step size selection and mini-batch sampling in determining convergence behavior.

## Key Contributions

1. SGD achieves ε-approximate stationary points in O(1/ε^4) iterations for general non-convex functions
2. Improved convergence rate of O(1/ε^2) under additional regularity conditions
3. Explicit characterization of convergence dependence on step size and mini-batch sampling

## Topics

`optimization` `machine-learning` `numerical-analysis` `non-convex-optimization` `stochastic-methods`

## Download PDF

The paper is automatically compiled on every push. Download the latest version:

**[Download paper.pdf](https://github.com/ChicagoHAI/math-convergence-rates-for-stochast-e8d8/releases/download/latest/paper.pdf)**

Or build it locally:

```bash
cd paper_draft
make          # Builds main.pdf
```

## Repository Structure

```
.
├── paper_draft/
│   ├── main.tex          # Main LaTeX file
│   ├── references.bib    # Bibliography
│   ├── sections/         # Paper sections
│   │   ├── abstract.tex
│   │   ├── introduction.tex
│   │   ├── preliminaries.tex
│   │   ├── main_results.tex
│   │   ├── applications.tex
│   │   └── conclusion.tex
│   └── commands/         # LaTeX macros
│       ├── math.tex
│       └── macros.tex
├── .github/workflows/    # Automatic PDF compilation
├── README.md
└── .math-agent/          # Generation metadata
```

## Building Locally

```bash
cd paper_draft
make          # Builds main.pdf
make clean    # Removes build artifacts
```

Or manually:
```bash
cd paper_draft
pdflatex main.tex
bibtex main
pdflatex main.tex
pdflatex main.tex
```

## Generated by

This paper was autonomously generated by the [Scibook Math Agent](https://scibook.ai).



---

*Generated on 2026-02-05*
