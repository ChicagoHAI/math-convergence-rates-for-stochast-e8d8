\begin{abstract}
We study the convergence rates of stochastic gradient descent (SGD) for non-convex optimization problems. While the behavior of SGD for convex optimization is well-understood, the non-convex setting presents unique challenges due to the presence of multiple local minima and saddle points. We establish new convergence rates under various smoothness and noise conditions, proving that SGD achieves an $\eps$-approximate stationary point in $O(1/\eps^4)$ iterations for general non-convex functions. For functions satisfying additional regularity conditions, we demonstrate improved rates of $O(1/\eps^2)$. Our analysis provides insights into the role of step size selection and mini-batch sampling in determining convergence behavior.
\end{abstract}